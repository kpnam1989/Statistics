{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: visualize the impact of trees and depth on out of sample scores\n",
    "Understand Out of Bag errors:\n",
    "https://datascience.stackexchange.com/questions/13151/randomforestclassifier-oob-scoring-method\n",
    "<br>\n",
    "\n",
    "Resources:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "<br>\n",
    "\n",
    "https://stats.stackexchange.com/questions/233275/multilabel-classification-metrics-on-scikit\n",
    "\n",
    "<br>\n",
    "https://stackoverflow.com/questions/31421413/how-to-compute-precision-recall-accuracy-and-f1-score-for-the-multiclass-case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, precision_recall_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "X, y = make_classification(n_features=30, \n",
    "                           random_state=0, n_classes=4, n_samples=4000, \n",
    "                           weights=[0.6, 0.1, 0.1, 0.2],\n",
    "                           n_informative=20)\n",
    "\n",
    "LABEL_SELECT = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test out some metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted mean  0.5599527100658053\n",
      "Unweighed mean of f1_score  0.3134842462398626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.64100486, 0.        , 0.        , 0.87583893]),\n",
       " array([0.99538591, 0.        , 0.        , 0.32503113]),\n",
       " array([0.77982254, 0.        , 0.        , 0.47411444]),\n",
       " array([2384,  403,  410,  803], dtype=int64))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    2384\n",
       "1     403\n",
       "2     410\n",
       "3     803\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth=4, n_estimators=20, n_jobs= -1)\n",
    "clf.fit(X, y)\n",
    "print(\"Weighted mean \", f1_score(y, clf.predict(X), average='weighted'))\n",
    "print(\"Unweighed mean of f1_score \",f1_score(y, clf.predict(X), average='macro'))\n",
    "display(precision_recall_fscore_support(y, clf.predict(X), average=None))\n",
    "print(\"Actual values\")\n",
    "display(pd.Series(y).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using balanced class weight: big difference for label 1-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted mean  0.6839775597338591\n",
      "Unweighed mean of f1_score  0.6052768443001172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.90764706, 0.4234104 , 0.40747029, 0.58096173]),\n",
       " array([0.64723154, 0.72704715, 0.58536585, 0.73723537]),\n",
       " array([0.75563173, 0.53515982, 0.48048048, 0.64983535]),\n",
       " array([2384,  403,  410,  803], dtype=int64))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth=4, n_estimators=20, n_jobs= -1,class_weight='balanced',random_state=0)\n",
    "clf.fit(X, y)\n",
    "print(\"Weighted mean \", f1_score(y, clf.predict(X), average='weighted'))\n",
    "print(\"Unweighed mean of f1_score \",f1_score(y, clf.predict(X), average='macro'))\n",
    "display(precision_recall_fscore_support(y, clf.predict(X), average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "applying threshold specifically for label 1, which I'm interested in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3634425183789126"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6479750778816199"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5161290322580645"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5745856353591159"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_proba = clf.predict_proba(X)\n",
    "y_1 = np.where(y == LABEL_SELECT , 1, 0)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_1, y_proba[:,LABEL_SELECT])\n",
    "f1 = 2 * np.multiply(precision, recall) / (precision + recall)\n",
    "selected_ind = np.argmax(f1)\n",
    "display(thresholds[selected_ind], precision[selected_ind], recall[selected_ind], f1[selected_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted mean  0.6835297221395317\n",
      "Unweighed mean of f1_score  0.6050624085431068\n",
      "Unwaited is lower because it treats the f1 on class 1-2-3 at the same weight as on class 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.90801887, 0.42241379, 0.40747029, 0.58096173]),\n",
       " array([0.64597315, 0.72952854, 0.58536585, 0.73723537]),\n",
       " array([0.75490196, 0.53503185, 0.48048048, 0.64983535]),\n",
       " array([2384,  403,  410,  803], dtype=int64))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Applying this threshold onto the prediction\n",
    "y_predict = clf.predict(X)\n",
    "y_predict_threshold = np.where(y_proba[:, LABEL_SELECT] >= thresholds[selected_ind], LABEL_SELECT, y_predict)\n",
    "print(\"Weighted mean \", f1_score(y, y_predict_threshold, average='weighted'))\n",
    "print(\"Unweighed mean of f1_score \",f1_score(y, y_predict_threshold, average='macro'))\n",
    "print(\"Unwaited is lower because it treats the f1 on class 1-2-3 at the same weight as on class 0\")\n",
    "display(precision_recall_fscore_support(y, y_predict_threshold, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No noticeable different in the metrics for all labels? So no need for a threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next\n",
    "Simple grid search. What is the most important metrics? f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://stackoverflow.com/questions/61965700/adjust-threshold-cros-val-score-sklearn\n",
    "\n",
    "class MyRF_multinomial(RandomForestClassifier):\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        RandomForestClassifier.fit(self, X=X, y=y, **kwargs)\n",
    "        y_proba = RandomForestClassifier.predict_proba(self, X=X)[:, LABEL_SELECT]\n",
    "        y_select = np.where(y == LABEL_SELECT, 1, 0)\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(y_select, y_proba)\n",
    "        f1 = 2 * np.multiply(precision, recall) / (precision + recall)\n",
    "        self.threshold = thresholds[np.argmax(f1)]\n",
    "        self.train_f1 = np.max(f1)\n",
    "        return self\n",
    "        \n",
    "    def predict(self,X, **kwargs):\n",
    "        result = RandomForestClassifier.predict(self, X=X)\n",
    "        \n",
    "        # Over-write the prediction with the specific threshold for for LABEL_SELECT\n",
    "        result_proba = RandomForestClassifier.predict_proba(self, X=X)[:, LABEL_SELECT]\n",
    "        predictions = np.where(result_proba >= self.threshold, LABEL_SELECT, result)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "def plot_res(final_res):\n",
    "    fig, axs = plt.subplots(figsize=(8,10), nrows=len(final_res.keys()), ncols=1)\n",
    "    \n",
    "    this_limit = [999, -1]\n",
    "    for this_index, this_key in enumerate(final_res.keys()):\n",
    "        tmp_min = min(np.min(final_res[this_key]), this_limit[0])\n",
    "        tmp_max = max(np.max(final_res[this_key]), this_limit[1])\n",
    "        this_limit = [tmp_min, tmp_max]\n",
    "    \n",
    "    this_limit = [this_limit[0] - 0.05, this_limit[1] + 0.05]\n",
    "    \n",
    "    for this_index, this_key in enumerate(final_res.keys()):\n",
    "        this_ax = axs[this_index]\n",
    "        this_val = final_res[this_key]\n",
    "        this_ax.plot(this_val)\n",
    "        this_ax.set_xticks(ticks=range(len(this_val)))\n",
    "        this_ax.set_xticklabels(labels=num_trees_range)\n",
    "        this_ax.set_ylim(this_limit)\n",
    "\n",
    "        this_ax.set_ylabel(\"Depth = {}\".format(this_key))\n",
    "    fig.tight_layout()\n",
    "\n",
    "def process_cv_results(clf, parameters):\n",
    "    results_df = pd.DataFrame(clf.cv_results_)\n",
    "\n",
    "    # Record the minimum of the out of sample score\n",
    "    split_test_score = [x for x in results_df.columns if '_test_score' in x and 'split' in x]\n",
    "    results_df['min_score'] = results_df[split_test_score].min(axis=1)\n",
    "    \n",
    "    column_list = [x for x in results_df.columns if 'param_' in x]\n",
    "\n",
    "    # param_cols = [x for x in results_df.columns if 'param_' in x]\n",
    "    results_subset = results_df[column_list + ['min_score']]\n",
    "    results_subset = results_subset.sort_values(by=column_list, ascending=True)\n",
    "    return results_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_args ={'labels':[LABEL_SELECT], 'average':'macro'}\n",
    "\n",
    "scorer_class1 = metrics.make_scorer(f1_score, **key_args) \n",
    "precision_class1= metrics.make_scorer(metrics.precision_score,**key_args) \n",
    "recall_class1= metrics.make_scorer(metrics.recall_score, **key_args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time 0.25318236351013185\n"
     ]
    }
   ],
   "source": [
    "# Source\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html#scoring\n",
    "# f1_macro: so weight the under-sampled class the same as the overwelming class\n",
    "# Custom scoring function\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "parameters = {'max_depth':[5,7, 10,15,20], \n",
    "              'n_estimators':[10, 20, 50,100, 200,500,1000],\n",
    "              'class_weight': [None,'balanced'],\n",
    "             'max_samples':[0.5, 0.75]} \n",
    "\n",
    "parameters = {'max_depth':[5,7], \n",
    "              'n_estimators':[10, 20],\n",
    "              'class_weight': [None,'balanced'],\n",
    "             'max_samples':[0.5, 0.75]} \n",
    "\n",
    "# clf = GridSearchCV(MyRF(), parameters)\n",
    "\n",
    "\n",
    "clf = GridSearchCV(RandomForestClassifier(n_jobs=-1), parameters,scoring=scorer_class1)\n",
    "start = time.time()\n",
    "clf.fit(X, y)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Run time {}\".format((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_samples</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>min_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>balanced</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>balanced</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>balanced</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>10</td>\n",
       "      <td>0.398148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>balanced</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>20</td>\n",
       "      <td>0.445714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>balanced</td>\n",
       "      <td>7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_class_weight param_max_depth param_max_samples param_n_estimators  \\\n",
       "8            balanced               5               0.5                 10   \n",
       "9            balanced               5               0.5                 20   \n",
       "10           balanced               5              0.75                 10   \n",
       "11           balanced               5              0.75                 20   \n",
       "12           balanced               7               0.5                 10   \n",
       "\n",
       "    min_score  \n",
       "8    0.352941  \n",
       "9    0.380952  \n",
       "10   0.398148  \n",
       "11   0.445714  \n",
       "12   0.450000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_subset = process_cv_results(clf, parameters)\n",
    "results_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are different to another level if I use f1_macro so my customer scorer works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Miniconda64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Miniconda64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Miniconda64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Miniconda64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Miniconda64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Miniconda64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Miniconda64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Miniconda64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_samples</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_f1_macro</th>\n",
       "      <th>...</th>\n",
       "      <th>std_test_precision_class1</th>\n",
       "      <th>rank_test_precision_class1</th>\n",
       "      <th>split0_test_recall_class1</th>\n",
       "      <th>split1_test_recall_class1</th>\n",
       "      <th>split2_test_recall_class1</th>\n",
       "      <th>split3_test_recall_class1</th>\n",
       "      <th>split4_test_recall_class1</th>\n",
       "      <th>mean_test_recall_class1</th>\n",
       "      <th>std_test_recall_class1</th>\n",
       "      <th>rank_test_recall_class1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.037402</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.110837</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'class_weight': None, 'max_depth': 5, 'max_sa...</td>\n",
       "      <td>0.320463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290593</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.019126</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.067005</td>\n",
       "      <td>0.003848</td>\n",
       "      <td>0.110683</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20</td>\n",
       "      <td>{'class_weight': None, 'max_depth': 5, 'max_sa...</td>\n",
       "      <td>0.322730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.489898</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009969</td>\n",
       "      <td>0.014572</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.036002</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.109089</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>10</td>\n",
       "      <td>{'class_weight': None, 'max_depth': 5, 'max_sa...</td>\n",
       "      <td>0.314326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411825</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.019877</td>\n",
       "      <td>0.020242</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.068605</td>\n",
       "      <td>0.003980</td>\n",
       "      <td>0.110211</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>20</td>\n",
       "      <td>{'class_weight': None, 'max_depth': 5, 'max_sa...</td>\n",
       "      <td>0.311398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.489898</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009907</td>\n",
       "      <td>0.009247</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.038403</td>\n",
       "      <td>0.001856</td>\n",
       "      <td>0.110377</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>None</td>\n",
       "      <td>7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'class_weight': None, 'max_depth': 7, 'max_sa...</td>\n",
       "      <td>0.411563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152294</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.098765</td>\n",
       "      <td>0.094105</td>\n",
       "      <td>0.040127</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.037402      0.000800         0.110837        0.001350   \n",
       "1       0.067005      0.003848         0.110683        0.001296   \n",
       "2       0.036002      0.001788         0.109089        0.001125   \n",
       "3       0.068605      0.003980         0.110211        0.000745   \n",
       "4       0.038403      0.001856         0.110377        0.000767   \n",
       "\n",
       "  param_class_weight param_max_depth param_max_samples param_n_estimators  \\\n",
       "0               None               5               0.5                 10   \n",
       "1               None               5               0.5                 20   \n",
       "2               None               5              0.75                 10   \n",
       "3               None               5              0.75                 20   \n",
       "4               None               7               0.5                 10   \n",
       "\n",
       "                                              params  split0_test_f1_macro  \\\n",
       "0  {'class_weight': None, 'max_depth': 5, 'max_sa...              0.320463   \n",
       "1  {'class_weight': None, 'max_depth': 5, 'max_sa...              0.322730   \n",
       "2  {'class_weight': None, 'max_depth': 5, 'max_sa...              0.314326   \n",
       "3  {'class_weight': None, 'max_depth': 5, 'max_sa...              0.311398   \n",
       "4  {'class_weight': None, 'max_depth': 7, 'max_sa...              0.411563   \n",
       "\n",
       "   ...  std_test_precision_class1  rank_test_precision_class1  \\\n",
       "0  ...                   0.290593                          16   \n",
       "1  ...                   0.489898                          12   \n",
       "2  ...                   0.411825                           9   \n",
       "3  ...                   0.489898                           5   \n",
       "4  ...                   0.152294                           4   \n",
       "\n",
       "   split0_test_recall_class1  split1_test_recall_class1  \\\n",
       "0                     0.0000                     0.0000   \n",
       "1                     0.0000                     0.0375   \n",
       "2                     0.0000                     0.0500   \n",
       "3                     0.0000                     0.0125   \n",
       "4                     0.0875                     0.0250   \n",
       "\n",
       "   split2_test_recall_class1  split3_test_recall_class1  \\\n",
       "0                   0.049383                   0.000000   \n",
       "1                   0.012346                   0.000000   \n",
       "2                   0.000000                   0.012346   \n",
       "3                   0.024691                   0.012346   \n",
       "4                   0.148148                   0.111111   \n",
       "\n",
       "   split4_test_recall_class1  mean_test_recall_class1  std_test_recall_class1  \\\n",
       "0                   0.012346                 0.012346                0.019126   \n",
       "1                   0.000000                 0.009969                0.014572   \n",
       "2                   0.037037                 0.019877                0.020242   \n",
       "3                   0.000000                 0.009907                0.009247   \n",
       "4                   0.098765                 0.094105                0.040127   \n",
       "\n",
       "   rank_test_recall_class1  \n",
       "0                       14  \n",
       "1                       15  \n",
       "2                       13  \n",
       "3                       16  \n",
       "4                       11  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_macro = GridSearchCV(RandomForestClassifier(n_jobs=-1,class_weight='balanced'), parameters,refit=False, scoring={'f1_macro':'f1_macro',\n",
    "                                                                               'f1_class1':scorer_class1,\n",
    "                                                                               'precision_class1': precision_class1,\n",
    "                                                                               'recall_class1': recall_class1\n",
    "                                                                                            })\n",
    "clf_macro.fit(X, y)\n",
    "# results_subset = process_cv_results(clf_macro, parameters)\n",
    "#results_subset.head()\n",
    "\n",
    "pd.DataFrame(clf_macro.cv_results_).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth':[5,7, 10,15,20], \n",
    "              'n_estimators':[10, 20, 50,100, 200,500,1000],\n",
    "             'max_samples':[0.5, 0.75]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time 3.9833602587382\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "clf_macro = GridSearchCV(RandomForestClassifier(n_jobs=-1,class_weight='balanced'), \n",
    "                         parameters,refit=False, scoring={'f1_macro':'f1_macro',\n",
    "                                                                               'f1_class1':scorer_class1,\n",
    "                                                                               'precision_class1': precision_class1,\n",
    "                                                                               'recall_class1': recall_class1\n",
    "                                                                                            })\n",
    "clf_macro.fit(X, y)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Run time {}\".format((end-start)/60))\n",
    "\n",
    "with open('./output_20Q3/1023_gridsearch_multi_rf.p', 'wb') as out_file:\n",
    "    pickle.dump(pd.DataFrame(clf_macro.cv_results_), out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "clf_multi = GridSearchCV(MyRF_multinomial(n_jobs=-1,class_weight='balanced'), \n",
    "                         parameters,refit=False, scoring={'f1_macro':'f1_macro',\n",
    "                                                          'f1_class1':scorer_class1,\n",
    "                                                          'precision_class1': precision_class1,\n",
    "                                                          'recall_class1': recall_class1\n",
    "                                                         })\n",
    "clf_multi.fit(X, y)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Run time {}\".format((end-start)/60))\n",
    "\n",
    "with open('./output_20Q3/1023_gridsearch_multi_custom.p', 'wb') as out_file:\n",
    "    pickle.dump(pd.DataFrame(clf_multi.cv_results_), out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_dict =['f1_macro','f1_class1'\n",
    "               ,'precision_class1',\n",
    "               'recall_class1']\n",
    "\n",
    "with open('./output_20Q3/1023_gridsearch_multi_rf.p', 'rb') as in_file:\n",
    "    results = pickle.load(in_file)\n",
    "\n",
    "drop_cols = []\n",
    "for this_col in results:\n",
    "    if 'time' in this_col or 'rank_test_' in this_col:\n",
    "        drop_cols.append(this_col)\n",
    "\n",
    "# calculate min of results\n",
    "for this_key in scoring_dict:\n",
    "    select_cols = [x for x in results.columns if \"_test_\" + this_key in x and 'mean_test_' + this_key not in x and 'std_test_' + this_key not in x]\n",
    "    results['min_test_' + this_key] = results[select_cols].min(axis=1)\n",
    "    \n",
    "results.drop(columns=drop_cols, axis=1).to_csv(\"./output_20Q3/1023_gridsearch_multi_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output_20Q3/1023_gridsearch_multi_custom.p', 'rb') as in_file:\n",
    "    results = pickle.load(in_file)\n",
    "drop_cols = []\n",
    "for this_col in results:\n",
    "    if 'time' in results or 'rank_test_' in results:\n",
    "        drop_cols.append(this_col)\n",
    "\n",
    "# calculate min of results\n",
    "for this_key in scoring_dict:\n",
    "    select_cols = [x for x in results.columns if \"_test_\" + this_key in x and 'mean_test_' + this_key not in x and 'std_test_' + this_key not in x]\n",
    "    results['min_test_' + this_key] = results[select_cols].min(axis=1)\n",
    "    \n",
    "results.drop(columns=drop_cols, axis=1).to_csv(\"./output_20Q3/1023_gridsearch_multi_custom.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
