{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was curious about sequence to sequence model after reading this article [here](https://weiminwang.blog/2017/09/29/multivariate-time-series-forecast-using-seq2seq-in-tensorflow/). So I examine the source code in Python and Tensorflow to explore the methodology.\n",
    "\n",
    "A quick summary:\n",
    "- **Weimin**'s article constructs the model from Tensorflow and uses the output of the *decoder* LSTM cell as the input for the next round\n",
    "- **Machinelearning Mastery** only outputs 1 word, but he then appends this word to the input and fits that into the model again. This is done outside of Keras and therefore does not enter the training process.\n",
    "- **Keras' official example** uses TimeDistributed and RepeatedVectors layers to achieve this.\n",
    "\n",
    "## Weimin's example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "https://github.com/aaxwaz/Multivariate-Time-Series-forecast-using-seq2seq-in-TensorFlow/blob/master/build_model_basic.py\n",
    "'''\n",
    "with variable_scope.variable_scope(scope or \"rnn_decoder\"):\n",
    "    state = initial_state\n",
    "    outputs = []\n",
    "    prev = None\n",
    "            \n",
    "    for i, inp in enumerate(decoder_inputs):\n",
    "        # enumerate through each input???\n",
    "        # i = index in each batch or the column in dex?\n",
    "        if loop_function is not None and prev is not None:\n",
    "            # this condition does not apply for the first one\n",
    "                  \n",
    "            with variable_scope.variable_scope(\"loop_function\", reuse=True):\n",
    "                inp = loop_function(prev, i)\n",
    "        if i > 0:\n",
    "            variable_scope.get_variable_scope().reuse_variables()\n",
    "        \n",
    "        output, state = cell(inp, state)\n",
    "        outputs.append(output)\n",
    "        if loop_function is not None:\n",
    "            prev = output\n",
    "            \n",
    "    return outputs, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code below is for the case that we have already processed the first input (prev is **not None**), we change the input using the loop function. In fact, you can see this in the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if loop_function is not None and prev is not None:\n",
    "    with variable_scope.variable_scope(\"loop_function\", reuse=True):\n",
    "        inp = loop_function(prev, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "loop_function: If not None, \n",
    "                    this function will be applied to the i-th output\n",
    "                                in order to generate the i+1-st input, \n",
    "                    and decoder_inputs will be **ignored**,\n",
    "                                except for the first element (\"GO\" symbol). \n",
    "            This can be used for decoding,\n",
    "              but also for training to emulate http://arxiv.org/abs/1506.03099\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that *prev* is the previous output, as we can see in the code. Basically, we feed the previous output back into our model to generate more output.\n",
    "\n",
    "This is different from a normal recurrent neural network, as you can see in the implementation below [source](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/): we feed an input X as a sequence into the RNN by 1 slice at a time. Each output is not related to the next input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Source: https://github.com/dennybritz/rnn-tutorial-rnnlm/blob/master/RNNLM.ipynb\n",
    "'''\n",
    "def forward_propagation(self, x):\n",
    "    # The total number of time steps\n",
    "    T = len(x)\n",
    "    \n",
    "    # During forward propagation we save all hidden states in s because need them later.\n",
    "    # We add one additional element for the initial hidden, which we set to 0\n",
    "    s = np.zeros((T + 1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "    \n",
    "    # The outputs at each time step. Again, we save them for later.\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "    return [o, s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning mastery\n",
    "To generate outputs in sequence, we can append the output as a new part of the input, as in this website [Machinelearning mastery](https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    \n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word (1 more word)\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        \n",
    "        # map the predicted word index to the one corresponding word\n",
    "        # Note: just 1 word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        \n",
    "        # append this word to the input text\n",
    "        # if the input text is too long, we truncate it from the left.\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "A simple way to take this into account in the modeling process can be achieved through Keras, as seen on Keras' website [here](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html). Here I reproduce the code for the trivial case [here](https://github.com/keras-team/keras/blob/master/examples/addition_rnn.py) with some modification for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabularty is  12\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_22 (SimpleRNN)    (None, 30)                1290      \n",
      "_________________________________________________________________\n",
      "repeat_vector_14 (RepeatVect (None, 4, 30)             0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_23 (SimpleRNN)    (None, 4, 40)             2840      \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 4, 12)             492       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 4, 12)             0         \n",
      "=================================================================\n",
      "Total params: 4,622\n",
      "Trainable params: 4,622\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Input shape is (None, 7, 12)\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.models import Sequential\n",
    "\n",
    "RNN = SimpleRNN\n",
    "\n",
    "DIGITS = 3\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "chars = '0123456789+ '\n",
    "print(\"Length of vocabularty is \",len(chars))\n",
    "\n",
    "def GenerateModel(RNN):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Decoder\n",
    "    model.add(RNN(30, input_shape=(MAXLEN, len(chars))))\n",
    "    model.add(layers.RepeatVector(DIGITS + 1))\n",
    "    \n",
    "    # Assume only 1 hidden layer for simplicity\n",
    "    model.add(RNN(40, return_sequences = True))\n",
    "        \n",
    "    model.add(layers.TimeDistributed(layers.Dense(len(chars))))\n",
    "    \n",
    "    model.add(layers.Activation('softmax'))\n",
    "    \n",
    "    # output can be 1 out of 12 so we use categorical crossenotry\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = GenerateModel(RNN)\n",
    "model.summary()\n",
    "print(\"Input shape is\", model.input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few basic things to note:\n",
    "- The vocabulary has 12 characters so each input (of the batch) is a sparse matrix of size 7 * 12 (maximum 7 characters)\n",
    "- The first layer, encoder layer, has 1290 parameters, implying 1290/30 = 43 parameters per RNN cell.\n",
    "- The layer second to last hold all outputs of the previous layers. Its output is of size 4 * 12 ( for 4 characters). With sigmoid activation, it will output 4 characters.\n",
    "\n",
    "The other interesting thing is:\n",
    "- *RepeatVector*, which has no parameter. This layer repeats the input for **DIGITS+1** times, as this is the maximum length of the ouput. Its input size is 30, which corresponds to 30 RNN cells of the previous layers. The choice of 30 is arbitrary. Hence the output shape is 4*30.\n",
    "- The layer after Repeat Vector has *return_sequences=True* and will return all the outputs instead of only the last one. The number of times it will output is 4, corresponding to the number of times the RepeatVector will repeat its inputs. Note: This should follow the Python RNN code above: for each input, there is only 1 output. <br> The output of this layer is 4*40, with 40 as an arbitrary value for the number of cells.\n",
    "- The TimeDistributed layer wraps around a Dense layer of 12 elements, and handles each time slice of its input separately. <br> Its input is 4x40. <br> Its output is 4x12, with the number of parameters is 492 = 12x(40 + 1) (+1 is for the bias), so we know that it will **share** the parameters among the Time Slices of the input.\n",
    "\n",
    "What happens if we set the number of hidden cells to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_24 (SimpleRNN)    (None, 1)                 14        \n",
      "_________________________________________________________________\n",
      "repeat_vector_15 (RepeatVect (None, 4, 1)              0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_25 (SimpleRNN)    (None, 4, 1)              3         \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 4, 12)             24        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 4, 12)             0         \n",
      "=================================================================\n",
      "Total params: 41\n",
      "Trainable params: 41\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def GenerateModel_Simple(RNN):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Decoder\n",
    "    model.add(RNN(1, input_shape=(MAXLEN, len(chars))))\n",
    "    model.add(layers.RepeatVector(DIGITS + 1))\n",
    "    \n",
    "    # Assume only 1 hidden layer for simplicity\n",
    "    model.add(RNN(1, return_sequences = True))\n",
    "        \n",
    "    model.add(layers.TimeDistributed(layers.Dense(len(chars))))\n",
    "    \n",
    "    model.add(layers.Activation('softmax'))\n",
    "    \n",
    "    # output can be 1 out of 12 so we use categorical crossenotry\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "GenerateModel_Simple(RNN).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for simple_rnn_24: number of paramters = 14 = 12 (# inputs) + 2 <br>\n",
    "For simple_rnn_25: number of parameters = 3 = 1 (# inputs) + 2 <br>\n",
    "Time distributed layer: 24 = 12 * (1 (# of inputs) + 1)\n",
    "\n",
    "## More advanced Keras\n",
    "This is also an example from Keras' official example [here](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html). Like Weimin's example, this model features an encoder layer, a decoder layer and a layer to generate target data. The process is as follows (from the website):\n",
    "- **encoder_input_data** is a 3D array of shape (num_pairs, max_english_sentence_length, num_english_characters)\n",
    "- **decoder_input_data** is a 3D array of shape (num_pairs, max_french_sentence_length, num_french_characters)\n",
    "- **decoder_target_data** is the same as decoder_input_data but offset by one timestep. decoder_target_data[:, t, :] will be the same as decoder_input_data[:, t + 1, :].\n",
    "\n",
    "The model uses teacher forcing method. According to the comment in the script, *teacher forcing* offsets the target sequence by one time step in the future. Its initial state are the state vectors from the encoder. Effectively, the decoder learns to generate targets[**t+1**...] given targets[...**t**], conditioned on the input sequence. \n",
    "\n",
    "We actually use the first character of the sequence with the start character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_seq[0, 0, target_token_index['\\t']] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training model illustrates 3 features of Keras' RNN:\n",
    "- The **return_state** contructor argument, configuring a RNN layer to return a list where the first entry is the outputs and the next entries are the internal RNN states. This is used to recover the states of the encoder.\n",
    "- The **inital_state** call argument, specifying the initial state(s) of a RNN. This is used to pass the encoder states to the decoder as **initial states**.\n",
    "- The **return_sequences** constructor argument, configuring a RNN to return its full sequence of outputs (instead of just the last output, which the defaults behavior). This is used in the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rom keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "\n",
    "# LSTM with return_state = true will return state_h, state_c and its output (possibly final output)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. \n",
    "# We don't use the return states in the training model\n",
    "# but we will use them in inference.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is then fitted onto the training data. The inference process will need some effort to be setup.\n",
    "- Create an encoder model from variable **encoder_inputs** and **encoder_states**\n",
    "- Create a decoder model that takes decoder_inputs + decoder_states_inputs, and output_decoder_outputs + decoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All the weights have been trained previously\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "...\n",
    "# decoder_lstm has been trained from before\n",
    "# it is set up to output the state and output a sequence\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The inference process looks like this\n",
    "states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "# And loop:\n",
    "# The target sequence will be updated\n",
    "# but the states value will remain unchanged in the loop\n",
    "output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
